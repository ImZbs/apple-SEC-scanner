{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImZbs/apple-SEC-scanner/blob/main/earnings_scanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d517e0e9",
        "outputId": "bae5d99f-5fae-4791-9498-c4d384b007c3"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install PyPDF2 transformers accelerate --quiet\n",
        "\n",
        "# Imports\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import json\n",
        "import torch\n",
        "import time\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "# Function to extract text from PDF pages\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text_pages = []\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            text_pages.append(text)\n",
        "    return text_pages\n",
        "\n",
        "# Chunk text by tokens\n",
        "def chunk_text_by_tokens(pages_text, tokenizer, max_tokens=1000):\n",
        "    current_chunk = []\n",
        "    current_token_count = 0\n",
        "    for page in pages_text:\n",
        "        page_tokens = tokenizer.encode(page, add_special_tokens=False)\n",
        "        if current_token_count + len(page_tokens) > max_tokens:\n",
        "            if current_chunk:\n",
        "                yield tokenizer.decode(sum(current_chunk, []), skip_special_tokens=True)\n",
        "            current_chunk = [page_tokens]\n",
        "            current_token_count = len(page_tokens)\n",
        "        else:\n",
        "            current_chunk.append(page_tokens)\n",
        "            current_token_count += len(page_tokens)\n",
        "    if current_chunk:\n",
        "        yield tokenizer.decode(sum(current_chunk, []), skip_special_tokens=True)\n",
        "\n",
        "# Load a smaller, faster BART model and tokenizer and pipeline\n",
        "def load_models():\n",
        "    model_name = \"sshleifer/distilbart-cnn-12-6\" # Using a faster model\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    # Move model to GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=0 if torch.cuda.is_available() else -1) # Use device argument\n",
        "    return tokenizer, model, device, qa_pipeline\n",
        "\n",
        "# Summarize text chunk using the faster BART model\n",
        "def summarize_text(text_chunk, tokenizer, model, device):\n",
        "    inputs = tokenizer([text_chunk], max_length=1024, return_tensors='pt', truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=4,\n",
        "        max_length=150,\n",
        "        min_length=40,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Main execution function to be used with %%capture\n",
        "def main_execution(pdf_path):\n",
        "    start_time = time.time()\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    extraction_time = time.time() - start_time\n",
        "    print(f\"Text extraction time: {extraction_time:.2f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    tokenizer, model, device, qa_pipeline = load_models()\n",
        "    chunks = list(chunk_text_by_tokens(pages_text, tokenizer, max_tokens=1000))\n",
        "    chunking_time = time.time() - start_time\n",
        "    print(f\"Chunking time: {chunking_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "    summaries = []\n",
        "    summarization_start_time = time.time()\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # print(f\"Summarizing chunk {i+1}/{len(chunks)}...\") # Suppress this output\n",
        "        try:\n",
        "            summary = summarize_text(chunk, tokenizer, model, device)\n",
        "            summaries.append(summary)\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing chunk {i+1}: {e}\")\n",
        "            summaries.append('') # Skip failed chunks\n",
        "\n",
        "    summarization_time = time.time() - summarization_start_time\n",
        "    print(f\"\\nTotal summarization time: {summarization_time:.2f} seconds\")\n",
        "\n",
        "    # Combine summaries for final output\n",
        "    final_summary = \" \".join([s for s in summaries if s]) # Filter out empty strings\n",
        "\n",
        "    # Define refined questions for the key insights you want to extract\n",
        "    refined_questions = [\n",
        "        \"What were the total net sales for the fiscal year?\",\n",
        "        \"What are the main revenue categories mentioned in the summary?\",\n",
        "        \"What are the primary risks to the company's business and financial condition?\",\n",
        "        \"Are there any mentions of intellectual property risks or patent claims?\",\n",
        "        \"What are the key competitive challenges the company faces?\",\n",
        "        \"Does the summary mention any information related to share repurchase programs or dividends?\",\n",
        "        \"What are the company's primary geographic markets for sales?\",\n",
        "    ]\n",
        "\n",
        "    # Use the question-answering pipeline to extract answers from the final summary\n",
        "    extracted_insights_refined = {}\n",
        "    for q in refined_questions:\n",
        "        # print(f\"Extracting insight for question: '{q}'...\") # Suppress this output\n",
        "        try:\n",
        "            answer = qa_pipeline(question=q, context=final_summary)\n",
        "            extracted_insights_refined[q] = answer['answer']\n",
        "            # print(f\"  Answer: {answer['answer']}\") # Suppress this output\n",
        "        except Exception as e:\n",
        "            print(f\"  Error extracting insight for question '{q}': {e}\")\n",
        "            extracted_insights_refined[q] = \"Could not extract.\"\n",
        "\n",
        "    # Format and print the organized key insights\n",
        "    formatted_insights = \"--- Organized Key Insights ---\\n\\n\"\n",
        "\n",
        "    for question, answer in extracted_insights_refined.items():\n",
        "        formatted_insights += f\"Q: {question}\\nA: {answer}\\n\\n\"\n",
        "\n",
        "    print(formatted_insights)\n",
        "\n",
        "# Execute the main function with %%capture\n",
        "#%%capture\n",
        "main_execution(\"/content/apple10k.pdf\")"
      ],
      "id": "d517e0e9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extraction time: 14.04 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking time: 13.75 seconds\n",
            "\n",
            "Total summarization time: 113.68 seconds\n",
            "--- Organized Key Insights ---\n",
            "\n",
            "Q: What were the total net sales for the fiscal year?\n",
            "A: 391,035 383,285 394,328\n",
            "\n",
            "Q: What are the main revenue categories mentioned in the summary?\n",
            "A: cash flow or fair value hedges\n",
            "\n",
            "Q: What are the primary risks to the company's business and financial condition?\n",
            "A: interest rates and foreign exchange rates\n",
            "\n",
            "Q: Are there any mentions of intellectual property risks or patent claims?\n",
            "A: The information is not incorporated by reference into this filing .\n",
            "\n",
            "Q: What are the key competitive challenges the company faces?\n",
            "A: competitors have aggressively cut prices and lowered product margins\n",
            "\n",
            "Q: Does the summary mention any information related to share repurchase programs or dividends?\n",
            "A: dividends\n",
            "\n",
            "Q: What are the company's primary geographic markets for sales?\n",
            "A: outside the U.S.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}