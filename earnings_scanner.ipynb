{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImZbs/apple-SEC-scanner/blob/main/earnings_scanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5dff574"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install PyPDF2 transformers accelerate --quiet\n",
        "\n",
        "# Imports\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import json\n",
        "import torch\n",
        "import time\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "# Function to extract text from PDF pages\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text_pages = []\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            text_pages.append(text)\n",
        "    return text_pages\n",
        "\n",
        "# Chunk text by tokens\n",
        "def chunk_text_by_tokens(pages_text, tokenizer, max_tokens=1000):\n",
        "    current_chunk = []\n",
        "    current_token_count = 0\n",
        "    for page in pages_text:\n",
        "        page_tokens = tokenizer.encode(page, add_special_tokens=False)\n",
        "        if current_token_count + len(page_tokens) > max_tokens:\n",
        "            if current_chunk:\n",
        "                yield tokenizer.decode(sum(current_chunk, []), skip_special_tokens=True)\n",
        "            current_chunk = [page_tokens]\n",
        "            current_token_count = len(page_tokens)\n",
        "        else:\n",
        "            current_chunk.append(page_tokens)\n",
        "            current_token_count += len(page_tokens)\n",
        "    if current_chunk:\n",
        "        yield tokenizer.decode(sum(current_chunk, []), skip_special_tokens=True)\n",
        "\n",
        "# Load a smaller, faster BART model and tokenizer\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\" # Using a faster model\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Summarize text chunk using the faster BART model\n",
        "def summarize_text(text_chunk, tokenizer, model, device):\n",
        "    inputs = tokenizer([text_chunk], max_length=1024, return_tensors='pt', truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=4,\n",
        "        max_length=150,\n",
        "        min_length=40,\n",
        "        length_penalty=2.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Main flow\n",
        "pdf_path = \"apple10k.pdf\"\n",
        "\n",
        "start_time = time.time()\n",
        "pages_text = extract_text_from_pdf(pdf_path)\n",
        "extraction_time = time.time() - start_time\n",
        "print(f\"Text extraction time: {extraction_time:.2f} seconds\")\n",
        "\n",
        "start_time = time.time()\n",
        "chunks = list(chunk_text_by_tokens(pages_text, tokenizer, max_tokens=1000))\n",
        "chunking_time = time.time() - start_time\n",
        "print(f\"Chunking time: {chunking_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "summaries = []\n",
        "summarization_start_time = time.time()\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "    try:\n",
        "        summary = summarize_text(chunk, tokenizer, model, device)\n",
        "        summaries.append(summary)\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing chunk {i+1}: {e}\")\n",
        "        summaries.append('') # Skip failed chunks\n",
        "\n",
        "summarization_time = time.time() - summarization_start_time\n",
        "print(f\"\\nTotal summarization time: {summarization_time:.2f} seconds\")\n",
        "\n",
        "# Combine summaries for final output\n",
        "final_summary = \" \".join([s for s in summaries if s]) # Filter out empty strings\n",
        "\n",
        "\n",
        "# Initialize a question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", device=0 if torch.cuda.is_available() else -1) # Use device argument\n",
        "\n",
        "# Define questions for the key insights you want to extract\n",
        "questions = [\n",
        "    \"What are the revenue jumps mentioned in the summary?\",\n",
        "    \"What are the cash-flow risks mentioned in the summary?\",\n",
        "    \"What are the buy-sell signals mentioned in the summary?\"\n",
        "]\n",
        "\n",
        "# Use the question-answering pipeline to extract answers from the final summary\n",
        "extracted_insights = {}\n",
        "for q in questions:\n",
        "    try:\n",
        "        answer = qa_pipeline(question=q, context=final_summary)\n",
        "        extracted_insights[q] = answer['answer']\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting insight for question '{q}': {e}\")\n",
        "        extracted_insights[q] = \"Could not extract.\"\n",
        "\n",
        "# Combine insights into a few sentences\n",
        "insight_sentences = []\n",
        "if extracted_insights.get(questions[0]) and extracted_insights[questions[0]] != \"Could not extract.\":\n",
        "    insight_sentences.append(f\"Regarding revenue jumps, the summary mentions: {extracted_insights[questions[0]]}.\")\n",
        "if extracted_insights.get(questions[1]) and extracted_insights[questions[1]] != \"Could not extract.\":\n",
        "    insight_sentences.append(f\"For cash-flow risks, the summary highlights: {extracted_insights[questions[1]]}.\")\n",
        "if extracted_insights.get(questions[2]) and extracted_insights[questions[2]] != \"Could not extract.\":\n",
        "    insight_sentences.append(f\"In terms of buy-sell signals, the summary notes: {extracted_insights[questions[2]]}.\")\n",
        "\n",
        "# Print the combined insights, wrapped\n",
        "print(\"--- Key Insights ---\")\n",
        "if insight_sentences:\n",
        "    combined_insights_text = \" \".join(insight_sentences)\n",
        "    wrapped_insights = textwrap.fill(combined_insights_text, width=80)\n",
        "    print(wrapped_insights)\n",
        "else:\n",
        "    print(\"Could not extract key insights from the summary.\")"
      ],
      "id": "d5dff574",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}